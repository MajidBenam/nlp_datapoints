{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "threaded-charity",
   "metadata": {},
   "source": [
    "# nlp_datapoints_jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-yacht",
   "metadata": {},
   "source": [
    "Read this to get an idea what I am going to doi:\n",
    "\n",
    "- I am going to bring in all the HTML files from seshat.info (Wiki) and seshatdatabrowser.\n",
    "\n",
    "* Wiki files will be from July 2022.\n",
    "* Databrowser files will be from Jan 2023.\n",
    "\n",
    "The idea is to build two separate list of the datapouints in both files:\n",
    "\n",
    "An NLP datapoint will have the following information:\n",
    "\n",
    "- 1  * polity_name (AfDurrn, YeZiyad, etc.)\n",
    "- 1  * var_name (Judges, Population, etc.) \n",
    "- 1+ * var_value (absent, unknown, present, 5000, etc.)  :: this will need cleaning.\n",
    "- 1  * is_it_on_wiki (True, False)\n",
    "- 1  * is_it_on_browser (True, False)\n",
    "- 1- * ref (Gakpo. 2006, Savbeti, 2004: The ups and downs of life, etc. )\n",
    "- 1- * ref_order_wiki (ref 13)\n",
    "- 1- * ref_order_browser (ref 16)\n",
    "- 1- * var_text (As poer our research there were Judges in AfdUrnn, etc)\n",
    "- 1- * var_ref_pages (13, 15-19)\n",
    "- 1- * ref_has_better_value (True, False)\n",
    "- 1- * ref_better_vale (Coady, 2006  ---> Lefts and Rights of Life, Coady, 2006, Rabbit Publications)\n",
    "- 1  * ref_has_visible_zotero (True, False)\n",
    "- 1  * ref_visible_zotero (\"BGSERZYQ\", etc. or \"HAS_NO_ZOTERO\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In order to get the job done, I need to bring the necessary files and the functions that I have written and used through time.\n",
    "\n",
    "- STEP0 (DONE): Bring in the Full HTML FILES\n",
    "- STEP1: Make simpler HTML files removing all the unneeded and noise data, and tagging the divs and all.\n",
    "- MAke sure you check for the last <p> tags that might appear at the end of HTML files and might contain useful information for expanding the shorter references: (Coady, 2006  ---> Lefts and Rights of Life, Coady, 2006, Rabbit Publications)\n",
    "- Before scraping the files, make sure you ignore values such as RA, Expert, etc, that do not have any meaningful scientific info in them.\n",
    "- Scrape the smaller HTML files, and look for these info:\n",
    "[polity_name, var_name, var_value, has_ref, ref_order, etc.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-brave",
   "metadata": {},
   "source": [
    "| **Website** | All Citations | Unique Citations | with visible Zotero: Unique (ALL)| with pages: Unique (ALL)| with ONE page: Unique (ALL)| with 2-5 pages: Unique (ALL)| with +5 pages: Unique (ALL)| Pers. Comms: Unique (ALL)| All Refs | Unique Citations |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| **Seshat.info (Wiki)** | **176145 **| **28390 **|**1455 (7831)**|**18005 (109750)**|**14072 (73170)**|**3548 (35234)**|**385 (1346)**|**191 (5605)**|****|****|\n",
    "| **Seshatbrowser** | **50192** | **14917** | **752 (2358)**|**10010 (31836)**|**8025 (25371)**|**1750 (6019)**|**235 (446)**|**126 (1543)**|x|y|\n",
    "\n",
    "\n",
    "| **Website** | All Citations | Unique Citations | Unique Refs | with visible Zotero: Unique (ALL)| with pages: Unique (ALL)| with ONE page: Unique (ALL)| with 2-5 pages: Unique (ALL)| with +5 pages: Unique (ALL)| Pers. Comms: Unique (ALL)|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| **Seshat.info (Wiki)** | **176145**| **28390**|**16876**|1455 **(7831)**|18005 **(109750)**|14072 **(73170)**|3548 **(35234)**|385 **(1346)**|191 **(5605)**|\n",
    "| **Seshatbrowser**|**50192**| **14917**|**8799**|752 **(2358)**|10010 **(31836)**|8025 **(25371)**|1750 **(6019)**|235 **(446)**|126 **(1543)**|\n",
    "\n",
    "\n",
    "\n",
    "#### `All Citations` Explanation\n",
    "- This is essentially equal to the number of  `NLP Points` because each use of a citation is an NLP point.  \n",
    "\n",
    "#### `Uniques with visible Zotero` Explanation\n",
    "- These are the ones with a clear Zotero link 100% certain. \n",
    "\n",
    "#### - On Browser: There are `328` children in the citations. In other words in `144` cases, [x] in references superscipt in the text, refers to two or more citations. A question is whether they add anything to the total number of citations or unique citations. Worth checking.\n",
    "\n",
    "#### - On Wiki: There are `494` children in the citations. In other words in `212` cases, [x] in references superscipt in the text, refers to two or more citations. A question is whether they add anything to the total number of citations or unique citations. Worth checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_folders: seshat_browser_Jan_30_2023    /    seshat_info_Jul_22 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "import time\n",
    "\n",
    "# try the other dic_finder... This is somehow for duplicate finders....\n",
    "# Do it for Browser:\n",
    "start_time = time.time()\n",
    "all_refs_browser, all_unique_refs_browser = z_helpers.ultimate_citation_dic_maker_duplicate_finder(\"seshat_browser_Jan_30_2023\", ALL_POLITIES=True)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_minutes = elapsed_time / 60\n",
    "\n",
    "print(\"Browser Work: --- %s minutes --- \" % elapsed_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "import time\n",
    "\n",
    "# Do it for Wiki\n",
    "start_time = time.time()\n",
    "all_refs_wiki, all_unique_refs_wiki = z_helpers.ultimate_citation_dic_maker_duplicate_finder(\"seshat_info_Jul_22\", ALL_POLITIES=True)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_minutes = elapsed_time / 60\n",
    "\n",
    "print(\"Wiki Work: --- %s minutes ---\" % elapsed_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For analyzing unique refs\n",
    "\n",
    "from python_files import z_helpers, main_scraper\n",
    "file_folder_wiki = \"seshat_info_Jul_22\"\n",
    "file_folder_browser = \"seshat_browser_Jan_30_2023\"\n",
    "unique_refs_augmented_wiki, meta_data_wiki = z_helpers.ultimate_ref_dic_maker_plus(file_folder_wiki)\n",
    "unique_refs_augmented_browser, meta_data_browser = z_helpers.ultimate_ref_dic_maker_plus(file_folder_browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For analyzing unique refs on browser\n",
    "\n",
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "#file_folder = \"seshat_info_Jul_22\"\n",
    "file_folder = \"seshat_browser_Jan_30_2023\"\n",
    "\n",
    "all_unique_refs = z_helpers.analyze_the_augmented_json_file(file_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For analyzing all refs on wiki\n",
    "\n",
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "file_folder = \"seshat_info_Jul_22\"\n",
    "#file_folder = \"seshat_browser_Jan_30_2023\"\n",
    "\n",
    "all_unique_refs = z_helpers.analyze_the_augmented_json_file(file_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in all_unique_refs[230:280]:\n",
    "    print(item)\n",
    "    print(\"______\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Barfield Thomas Afghanistan A Cultural and Political History',\n",
    "'Barfield Thomas Afghanistan a cultural and political history',\n",
    "'(Barfield 2010) Thomas Barfield 2010 Afghanistan a cultural and political history Princeton University Press',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "augmented_json_folder = \"seshat_browser_Jan_30_2023\"\n",
    "augmented_json_file = \"a_dic_with_info_on_children_for_\" + augmented_json_folder + \".json\"\n",
    "with open(augmented_json_file) as json_file:\n",
    "    refs_dic = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_json_folder = \"seshat_browser_Jan_30_2023\"\n",
    "\n",
    "refs_json_file = \"a_dic_with_info_on_children_for_\" + refs_json_folder + \".json\"\n",
    "with open(refs_json_file) as json_file:\n",
    "    refs_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-sucking",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_columns = [\"polity\", \"variable\", \"page_from\", \"page_to\", \"trimmed\", \"original\", \"trimplus\"]\n",
    "long_df = pd.DataFrame(columns = df_columns)\n",
    "\n",
    "for kk, vv in refs_data.items():\n",
    "    for values in vv:\n",
    "        my_polity = kk.split(\"_\")[-1]\n",
    "        zotero_id = \"HAS_NO_ZOTERO_YET\"\n",
    "        if values[\"zoteroID\"]:\n",
    "            zotero_id = values[\"zoteroID\"][0]\n",
    "        new_values_dic = {\n",
    "                           \"polity\":  my_polity,\n",
    "                           \"variable\":  \"xxx\",\n",
    "                           \"page_from\":  values.get(\"page_from\"),\n",
    "                           \"page_to\":  values.get(\"page_to\"),\n",
    "                            #\"zotero\": zotero_id ,\n",
    "                           \"trimmed\":  values[\"trimmedText\"],\n",
    "                            \"original\": values[\"originalText\"], \n",
    "            \"trimplus\": values[\"trimmedTextPLUS\"],\n",
    "                        }\n",
    "        long_df = long_df.append(new_values_dic, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-brown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Set the text alignment for the columns\n",
    "short_df = long_df[[\"polity\", \"page_from\", \"page_to\", \"original\", \"trimmed\", \"trimplus\"]]\n",
    "short_df.style.set_properties(**{'text-align': 'left'})\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Calculates the Jaccard similarity between two strings.\n",
    "    \n",
    "    Args:\n",
    "    str1 (str): The first string to compare.\n",
    "    str2 (str): The second string to compare.\n",
    "    \n",
    "    Returns:\n",
    "    float: A value between 0 and 1 representing the Jaccard similarity between the two strings.\n",
    "    \"\"\"\n",
    "    str1 = re.sub(r'\\W+', ' ', str1).lower()\n",
    "    str2 = re.sub(r'\\W+', ' ', str2).lower()\n",
    "    a = set(str1.split())\n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    #print(str1)\n",
    "    #print(str2)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "for index, row in short_df.iterrows():\n",
    "    similarity = jaccard_similarity(row['trimplus'], \"Avari, Burjor. India: the ancient past: a history of the Indian sub-continent from c. 7000 BC to AD 1200. 2007,\")\n",
    "    if similarity > 0.6:\n",
    "        print(index, f\": ({similarity} and {row['polity']})\", row['trimplus'])\n",
    "#filtered_df = short_df[short_df['trimplus'].str.len() < 80 ]\n",
    "\n",
    "\n",
    "#filtered_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-chemical",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls = 0\n",
    "for kk, vv in refs_dic.items():\n",
    "    for item in vv:\n",
    "        if item[\"trimmedText\"].lower().count('; ')>2 and \"&amp;\" not in item[\"trimmedText\"].lower() and \"for a brief look at the decline\" not in item[\"trimmedText\"].lower():\n",
    "            urls+=1\n",
    "            print(item)\n",
    "            print(f\"______{kk}______\")\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Calculates the Jaccard similarity between two strings.\n",
    "    \n",
    "    Args:\n",
    "    str1 (str): The first string to compare.\n",
    "    str2 (str): The second string to compare.\n",
    "    \n",
    "    Returns:\n",
    "    float: A value between 0 and 1 representing the Jaccard similarity between the two strings.\n",
    "    \"\"\"\n",
    "    str1 = re.sub(r'\\W+', ' ', str1).lower()\n",
    "    str2 = re.sub(r'\\W+', ' ', str2).lower()\n",
    "    a = set(str1.split())\n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    print(str1)\n",
    "    print(str2)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "str1 = \"Barfield Thomas\"\n",
    "str2 = \"(Barfield 2010) Thomas\"\n",
    "\n",
    "print(\"Jaccard similarity: {:.5f}\".format(jaccard_similarity(str1, str2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-physics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-guide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-kingston",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "z_helpers.check_page_finders_checklist(z_helpers.check_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSON file into a pandas DataFrame\n",
    "df = pd.read_json('only_unique_citations_for_seshat_browser_Jan_30_2023.json')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "my_str = \"d Capiven: Yale University Press, 357.)\"\n",
    "a,b, c = z_helpers.find_pages_in(my_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-doctor",
   "metadata": {},
   "source": [
    "# Let's run the full nlp analysis\n",
    "\n",
    "### Improve the quality of the code and put it in a function so that we can take care of the most number of shorties and add second and third guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "\n",
    "#root_dir = \"seshat_browser_Jan_30_2023\"\n",
    "root_dir = \"seshat_info_Jul_22\"\n",
    "\n",
    "main_scraper.nlp_vars_html_extractor(root_dir, ALL_POLITIES=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"json_files/extra_ps_at_the_end_for_seshat_info_Jul_22.json\") as json_file:\n",
    "    extras = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"a_dic_with_info_on_children_for_seshat_info_Jul_22.json\") as json_file:\n",
    "    details = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk, vv in extras.items():\n",
    "    for pot_v in vv:\n",
    "        if \"McEvedy\" in pot_v:\n",
    "            print(kk, pot_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in details.items():\n",
    "    for index, item in enumerate(value):\n",
    "        if item[\"originalText\"] == \"SAME_AS_TRIMMED\":\n",
    "            x = item[\"trimmedText\"]\n",
    "        else:   \n",
    "            x = item[\"originalText\"]\n",
    "        if len(x) < 50  and len(x) >40 and item[\"hasPersonalComment\"] ==False:\n",
    "            print(x)\n",
    "            #print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-looking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-davis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "\n",
    "shorties = 0\n",
    "good_shorties = 0\n",
    "good_shorties_all = 0\n",
    "two_hits = 0\n",
    "\n",
    "all_the_ps_in_all_polities = [] \n",
    "for kk, vv in extras.items():\n",
    "    for a_thing in vv:\n",
    "        all_the_ps_in_all_polities.append(a_thing)\n",
    "for key, value in details.items():\n",
    "    polity_name = key.split(\"_\")[-1]\n",
    "    #if polity_name == \"CnNWei*\":\n",
    "    #    print(key)\n",
    "    for index, item in enumerate(value):\n",
    "        if item[\"originalText\"] == \"SAME_AS_TRIMMED\":\n",
    "            x = item[\"trimmedText\"]\n",
    "        else:   \n",
    "            x = item[\"originalText\"]\n",
    "        #if polity_name == \"CnNWei*\":\n",
    "        #    print(x)\n",
    "        #    print(\"***********\")\n",
    "        if len(x) < 40:\n",
    "            shorties+=1\n",
    "            # find the year\n",
    "            the_year_plus_in_shortie =  z_helpers.find_the_year_plus(x)\n",
    "            the_top_ws_in_shortie =  z_helpers.first_three_words_finder(x)\n",
    "            the_ps_below = extras[polity_name]\n",
    "            \n",
    "            potential_hits = []\n",
    "            for a_p in the_ps_below:\n",
    "                number_of_hits = 0\n",
    "                num_of_words = len(the_top_ws_in_shortie)\n",
    "                for a_w in the_top_ws_in_shortie:\n",
    "                    if the_year_plus_in_shortie and the_year_plus_in_shortie[0] in a_p and len(a_w) >= 3 and a_w.lower() in a_p.lower():\n",
    "                        #print(f\"HIT: {x}  ----->  in {key} \\n {a_p}***\")\n",
    "                        #print(\"_____________\")\n",
    "                        number_of_hits+=1\n",
    "                        #potential_hits.append(a_p)\n",
    "                        #details[key][index][\"second_chance\"] = a_p\n",
    "                if number_of_hits == num_of_words:\n",
    "                    # high probability hit:\n",
    "                    potential_hits.append(a_p)\n",
    "                    #details[key][index][\"second_chance_high\"] = a_p\n",
    "                elif number_of_hits == 2 and num_of_words ==3 and a_p not in potential_hits:\n",
    "                    # 66% highly probale\n",
    "                    potential_hits.append(a_p)\n",
    "                    #details[key][index][\"second_chance_medium\"] = a_p\n",
    "            if len(potential_hits) == 1:\n",
    "                # Very likely hit:\n",
    "                good_shorties+=1\n",
    "                details[key][index][\"second_chance_high\"] = a_p\n",
    "            elif len(potential_hits) == 0:\n",
    "                print(f\"NOOOOOOOOOOOO HITs in {key}: {potential_hits}\")\n",
    "                two_hits+=1\n",
    "                print(\"_____________\")\n",
    "                    \n",
    "            \n",
    "            # We need to fo a similar approach for third hits (from all polities)        \n",
    "            for a_p in all_the_ps_in_all_polities:\n",
    "                for a_w in the_top_ws_in_shortie:\n",
    "                    if the_year_plus_in_shortie and \"second_chance\" not in details[key][index].keys() and the_year_plus_in_shortie[0] in a_p and len(a_w) > 4 and a_w.lower() in a_p.lower():\n",
    "                        #print(f\"HIT  +++++ : {x}  ----->  in {key} \\n {a_p}***\")\n",
    "                        #print(\"_____________\")\n",
    "                        good_shorties_all+=1\n",
    "                        details[key][index][\"second_chance_from_all_pols\"] = a_p\n",
    "                        break            #print(x)\n",
    "\n",
    "print(good_shorties, \" out of \", shorties, \" had a Hit.\")\n",
    "print(two_hits, \" out of \", shorties, \" had two Hits.\")\n",
    "\n",
    "print(good_shorties_all, \" out of \", shorties, \" had a third Hit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "badies = 0\n",
    "for key, value in details.items():\n",
    "    for item in value:\n",
    "        if len(item[\"trimmedText\"]) < 40 and \"second_chance\" in item.keys() and \"second_chance_from_all_pols\" in item.keys():\n",
    "            print(key, item)\n",
    "            badies+=1\n",
    "print(badies)\n",
    "#details[\"REF_567_CnEHan*\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-occasion",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "\n",
    "file_dir = \"seshat_browser_Jan_30_2023\"\n",
    "#file_dir = \"seshat_info_Jul_22\"\n",
    "my_details = z_helpers.find_second_and_third_guess(file_dir, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"citations_of_max_length_50_with_second_and_third_chances_for_seshat_info_Jul_22.json\") as f1:\n",
    "    xxxx = json.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for kk, vv in xxxx.items():\n",
    "    if count < 100:\n",
    "        print(kk)\n",
    "        print(vv)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for kk, vv in my_details.items():\n",
    "    for pot_v in vv:\n",
    "        if \"second_chance\" not in pot_v.keys() and \"third_chance\" not in pot_v.keys() and len(pot_v[\"trimmedText\"]) < 50:\n",
    "            print(kk)\n",
    "            print(pot_v[\"trimmedText\"])\n",
    "            count+=1\n",
    "            print(\"_____\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk, vv in my_details.items():\n",
    "    for pot_v in vv:\n",
    "        if \"second_chance\" in pot_v.keys() or \"third_chance\" in pot_v.keys():\n",
    "            print(kk)\n",
    "            print(vv)\n",
    "            print(\"_____\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [\"b\", \"c\", \"d\"]\n",
    "a[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Gabriel (2003:)\"\n",
    "\"(Lewis 1999b)\"\n",
    "\"(Lorge, 2012)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def first_n_words_finder(a_str, n):\n",
    "    \"\"\"\n",
    "    Finds the first three words in a string (or the first two, or the first one)\n",
    "    \"\"\"\n",
    "    regex = r\"\\b([a-zA-Z\\u00C0-\\u017F']+)\\b(?:\\W+\\b([a-zA-Z\\u00C0-\\u017F']+)\\b)?(?:\\W+\\b([a-zA-Z\\u00C0-\\u017F']+)\\b)?(?:\\W+\\b([a-zA-Z\\u00C0-\\u017F']+)\\b)?\"\n",
    "    match = re.search(regex, a_str)\n",
    "    if match:\n",
    "        words = [match.group(i) for i in range(1, n+1) if match.group(i)]\n",
    "        return words\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_n_words_finder(\"(Lewüis ali maün, neda 1999b)\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-receiver",
   "metadata": {},
   "source": [
    "# Mapping between refs and vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "file_dir = \"seshat_info_Jul_22\"\n",
    "\n",
    "my_mappings_wiki = main_scraper.map_ref_to_var(file_dir, ALL_POLITIES=True)\n",
    "my_reverse_dic_wiki = main_scraper.find_var_occurrences_with_refs(file_dir)\n",
    "\n",
    "file_dir = \"seshat_browser_Jan_30_2023\"\n",
    "\n",
    "my_mappings_browser = main_scraper.map_ref_to_var(file_dir, ALL_POLITIES=True)\n",
    "my_reverse_dic_browser = main_scraper.find_var_occurrences_with_refs(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_reverse_dic_browser[\"Language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-pottery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "#file_dir = \"seshat_info_Jul_22\"\n",
    "file_dir = \"seshat_browser_Jan_30_2023\"\n",
    "zotero_json_file = \"Seshat_Databank_jan_23.json\"\n",
    "my_ref_zot_id_dic = main_scraper.make_99_percent_zotero_guesses(file_dir, zotero_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ref_zot_id_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "file_dir = \"seshat_info_Jul_22\"\n",
    "#file_dir = \"seshat_browser_Jan_30_2023\"\n",
    "\n",
    "main_scraper.html_maker_nlp(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"alinji\"[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-message",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#len(my_reverse_dic[\"human sacrifice of a relative\"])\n",
    "\n",
    "all_my_rows = [\"\"\"\n",
    "{% extends \"core/seshat-base.html\" %}\n",
    "{% load static %}\n",
    "{% load humanize %}\n",
    "\n",
    "{% block content %}\n",
    "<div class=\"container\">\n",
    "<h1 class=\"pt-3 \"> NLP Datapoints</h1>\n",
    "<h3 class=\"pt-1\"> This is the room for our more discussions on NLP.</h3>\n",
    "<h2><span class=\"badge bg-primary\">Case 1: Numeric Values</span></h2>\n",
    "<div class=\"row\">\n",
    "    <div class=\"table-responsive col-md-12\">\n",
    "        <table id=\"table_id\" class=\"table align-middle table-hover table-striped table-bordered\" style=\"padding: 0.25 rem !important;\">\n",
    "            <thead>\n",
    "            <tr>\n",
    "                <th scope=\"col\" class=\"text-secondary\">#</th>\n",
    "                <th class=\"col-md-2\" scope=\"col\" style=\"text-align: left\" class=\"fw-light\"> \n",
    "                    Variable </th>\n",
    "                <th scope=\"col\" style=\"text-align: center\" class=\"fw-light\">\n",
    "                Citations\n",
    "                </th>\n",
    "                <th scope=\"col\" style=\"text-align: center\" class=\"fw-light\">have Zotero?</th>\n",
    "                <th scope=\"col\" style=\"text-align: center\" class=\"fw-light\">have pages?</th>\n",
    "            </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "\"\"\",]\n",
    "count = 1\n",
    "for kk, vv in my_reverse_dic_browser.items():\n",
    "    # check we have things on browser:\n",
    "    data_on_browser = my_reverse_dic_browser.get(kk)\n",
    "    if data_on_browser is None:\n",
    "        data_on_browser = \"\"\n",
    "    my_template = f\"\"\"\n",
    "                <tr>\n",
    "                    <td class=\"text-secondary\" scope=\"row\">{count}</td>\n",
    "                    <td class=\"fw-bold\">{kk}</td>\n",
    "                    <td  class=\"fw-bold\">{len(vv)} ({len(data_on_browser)})</td>\n",
    "                    <td></td>\n",
    "                    <td></td>\n",
    "                </tr>\"\"\"\n",
    "    count+=1\n",
    "    all_my_rows.append(my_template)\n",
    "    \n",
    "all_my_rows.append(\"\"\"\n",
    "            </tbody>\n",
    "        </table>\n",
    "    </div>\n",
    "  \n",
    "</div>\n",
    "\n",
    "{% endblock %}\n",
    "\"\"\")\n",
    "full_string = \"\".join(all_my_rows)\n",
    "with open(f\"to_be_transported_to_django.html\", \"w\", encoding='utf-8') as fw:\n",
    "    fw.write(full_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "source=\"♠ali yaran♣   ,knihji MAJIDBENAM_REF_1_   ,kn ♠ali ttttyaran♣   ,knihji MAJIDBENAM_REF_51_   ihj  ♠ali yazzzzuuuuuuuuuuran♣ i MAJIDBENAM_REF ♠ali yazzzzran♣  _34_   ,knihji MAJIDBENAM_REF_3_\"\n",
    "ref_regex = re.compile(\"MAJIDBENAM_REF_(\\d{1,4})_\")\n",
    "catches_all = ref_regex.finditer(source)\n",
    "#print(catches_all)\n",
    "if catches_all:\n",
    "    for index, catches in enumerate(catches_all):\n",
    "        # text to be used for splitting\n",
    "        text_for_split = f\"MAJIDBENAM_REF_{catches.group(1)}_\"\n",
    "        var_part = source.split(text_for_split)[0]\n",
    "        var_regex = re.compile(\"♠(.*?)♣\")\n",
    "        last_match = None\n",
    "        for match in re.finditer(var_regex, var_part):\n",
    "            last_match = match\n",
    "        print(f\"{text_for_split}: \", last_match.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-intersection",
   "metadata": {},
   "source": [
    "# Let's fill up the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"a_dic_with_info_on_children_for_seshat_browser_Jan_30_2023.json\", \"r\") as my_file:\n",
    "    full_info_dic = json.load(my_file)\n",
    "\n",
    "# surprisingly, this is the one that has all the keys\n",
    "with open(\"all_citations_with_duplicates_indicated_for_seshat_browser_Jan_30_2023.json\", \"r\") as my_file:\n",
    "    dupl_info_dic = json.load(my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_info_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dupl_info_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-tragedy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_info_dic[\"REF_6_AfDurrn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_info_dic[\"REF_2_AfDurrn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupl_info_dic[\"REF_6_AfDurrn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupl_info_dic[\"REF_2_AfDurrn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "'IS_DUPLICATE_REF_1_AfDurrn'[13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_info_dic[\"REF_128_CnErlig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupl_info_dic[\"REF_133_CnErlig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_info_dic[\"REF_20_IrSasn2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupl_info_dic[\"REF_20_IrSasn2\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-calendar",
   "metadata": {},
   "source": [
    "# Create Full NLP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_files_root_dir = \"seshat_browser_Jan_30_2023\"\n",
    "\n",
    "with open(f\"all_citations_with_duplicates_indicated_for_{html_files_root_dir}.json\", \"r\") as my_file:\n",
    "    dupl_info_dic = json.load(my_file)\n",
    "with open(f\"a_dic_with_info_on_children_for_{html_files_root_dir}.json\", \"r\") as my_file:\n",
    "    full_info_dic = json.load(my_file)\n",
    "\n",
    "#make a simple REF_123_AfdUrrn --- > Coady Bads and goodss of life 23-24:\n",
    "ref_number_ref_text_mapper = {}\n",
    "for k_all, v_all in dupl_info_dic.items():\n",
    "    if \"IS_DUPLICATE_\" in v_all:\n",
    "        # extract the mother of this\n",
    "        mother_ref = v_all[13:]\n",
    "        for index1, a_ref in enumerate(full_info_dic[mother_ref]):\n",
    "            ref_number_ref_text_mapper[k_all + \"_\" + str(index1+1)] = [ a_ref[\"originalText\"], a_ref[\"trimmedText\"]]\n",
    "    # if it is not a duplicate, still go and get the data from the other dic\n",
    "    else:\n",
    "        for index2, a_ref in enumerate(full_info_dic[k_all]):\n",
    "            ref_number_ref_text_mapper[k_all+ \"_\" + str(index2+1)] = [ a_ref[\"originalText\"], a_ref[\"trimmedText\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_number_ref_text_mapper.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-eleven",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exceptional-dayton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "#file_dir = \"seshat_info_Jul_22\"\n",
    "file_dir = \"seshat_browser_Jan_30_2023\"\n",
    "\n",
    "my_full_nlp_json = main_scraper.nlp_full_datapoint_extarctor(file_dir, ALL_POLITIES=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wired-seven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'variable': 'Moat',\n",
       " 'value': 'inferred present',\n",
       " 'polity': 'AfDurrn',\n",
       " 'description': '\"Built on the grand scale by Ahmad Shah Durrani - the dashing young cavalryman who founded the great Durrani Empire - with huge walls surrounded by a moat and pierced by six massive gates, Kandahar was designed to impress the approaching traveller, friend or foe. The walls were pulled down in the 1940s...\"',\n",
       " 'citation_text': '(Gall 2012, 19) Sandy Gall. 2012. War Against the Taliban: Why It All Went Wrong in Afghanistan. Bloomsbury. London.',\n",
       " 'citation_text_trimmed': '(Gall 2012) Sandy Gall. 2012. War Against the Taliban: Why It All Went Wrong in Afghanistan. Bloomsbury. London.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_full_nlp_json[\"REF_98_AfDurrn_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_full_nlp_json[\"REF_6_AfDurrn_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-writing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
