{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "threaded-charity",
   "metadata": {},
   "source": [
    "# nlp_datapoints_jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-yacht",
   "metadata": {},
   "source": [
    "Read this to get an idea what I am going to doi:\n",
    "\n",
    "- I am going to bring in all the HTML files from seshat.info (Wiki) and seshatdatabrowser.\n",
    "\n",
    "* Wiki files will be from July 2022.\n",
    "* Databrowser files will be from Jan 2023.\n",
    "\n",
    "The idea is to build two separate list of the datapouints in both files:\n",
    "\n",
    "An NLP datapoint will have the following information:\n",
    "\n",
    "- 1  * polity_name (AfDurrn, YeZiyad, etc.)\n",
    "- 1  * var_name (Judges, Population, etc.) \n",
    "- 1+ * var_value (absent, unknown, present, 5000, etc.)  :: this will need cleaning.\n",
    "- 1  * is_it_on_wiki (True, False)\n",
    "- 1  * is_it_on_browser (True, False)\n",
    "- 1- * ref (Gakpo. 2006, Savbeti, 2004: The ups and downs of life, etc. )\n",
    "- 1- * ref_order_wiki (ref 13)\n",
    "- 1- * ref_order_browser (ref 16)\n",
    "- 1- * var_text (As poer our research there were Judges in AfdUrnn, etc)\n",
    "- 1- * var_ref_pages (13, 15-19)\n",
    "- 1- * ref_has_better_value (True, False)\n",
    "- 1- * ref_better_vale (Coady, 2006  ---> Lefts and Rights of Life, Coady, 2006, Rabbit Publications)\n",
    "- 1  * ref_has_visible_zotero (True, False)\n",
    "- 1  * ref_visible_zotero (\"BGSERZYQ\", etc. or \"HAS_NO_ZOTERO\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In order to get the job done, I need to bring the necessary files and the functions that I have written and used through time.\n",
    "\n",
    "- STEP0 (DONE): Bring in the Full HTML FILES\n",
    "- STEP1: Make simpler HTML files removing all the unneeded and noise data, and tagging the divs and all.\n",
    "- MAke sure you check for the last <p> tags that might appear at the end of HTML files and might contain useful information for expanding the shorter references: (Coady, 2006  ---> Lefts and Rights of Life, Coady, 2006, Rabbit Publications)\n",
    "- Before scraping the files, make sure you ignore values such as RA, Expert, etc, that do not have any meaningful scientific info in them.\n",
    "- Scrape the smaller HTML files, and look for these info:\n",
    "[polity_name, var_name, var_value, has_ref, ref_order, etc.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-brave",
   "metadata": {},
   "source": [
    "| **Website** | All Citations | Unique Citations | with visible Zotero: Unique (ALL)| with pages: Unique (ALL)| with ONE page: Unique (ALL)| with 2-5 pages: Unique (ALL)| with +5 pages: Unique (ALL)| Pers. Comms: Unique (ALL)| All Refs | Unique Citations |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| **Seshat.info (Wiki)** | **176145 **| **28390 **|**1455 (7831)**|**18005 (109750)**|**14072 (73170)**|**3548 (35234)**|**385 (1346)**|**191 (5605)**|****|****|\n",
    "| **Seshatbrowser** | **50192** | **14917** | **752 (2358)**|**10010 (31836)**|**8025 (25371)**|**1750 (6019)**|**235 (446)**|**126 (1543)**|x|y|\n",
    "\n",
    "\n",
    "| **Website** | All Citations | Unique Citations | Unique Refs | with visible Zotero: Unique (ALL)| with pages: Unique (ALL)| with ONE page: Unique (ALL)| with 2-5 pages: Unique (ALL)| with +5 pages: Unique (ALL)| Pers. Comms: Unique (ALL)|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| **Seshat.info (Wiki)** | **176145**| **28390**|**16876**|1455 **(7831)**|18005 **(109750)**|14072 **(73170)**|3548 **(35234)**|385 **(1346)**|191 **(5605)**|\n",
    "| **Seshatbrowser**|**50192**| **14917**|**8799**|752 **(2358)**|10010 **(31836)**|8025 **(25371)**|1750 **(6019)**|235 **(446)**|126 **(1543)**|\n",
    "\n",
    "\n",
    "\n",
    "#### `All Citations` Explanation\n",
    "- This is essentially equal to the number of  `NLP Points` because each use of a citation is an NLP point.  \n",
    "\n",
    "#### `Uniques with visible Zotero` Explanation\n",
    "- These are the ones with a clear Zotero link 100% certain. \n",
    "\n",
    "#### - On Browser: There are `328` children in the citations. In other words in `144` cases, [x] in references superscipt in the text, refers to two or more citations. A question is whether they add anything to the total number of citations or unique citations. Worth checking.\n",
    "\n",
    "#### - On Wiki: There are `494` children in the citations. In other words in `212` cases, [x] in references superscipt in the text, refers to two or more citations. A question is whether they add anything to the total number of citations or unique citations. Worth checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_folders: seshat_browser_Jan_30_2023    /    seshat_info_Jul_22 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "import time\n",
    "\n",
    "# try the other dic_finder... This is somehow for duplicate finders....\n",
    "# Do it for Browser:\n",
    "start_time = time.time()\n",
    "all_refs_browser, all_unique_refs_browser = z_helpers.ultimate_citation_dic_maker_duplicate_finder(\"seshat_browser_Jan_30_2023\", ALL_POLITIES=True)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_minutes = elapsed_time / 60\n",
    "\n",
    "print(\"Browser Work: --- %s minutes --- \" % elapsed_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "import time\n",
    "\n",
    "# Do it for Wiki\n",
    "start_time = time.time()\n",
    "all_refs_wiki, all_unique_refs_wiki = z_helpers.ultimate_citation_dic_maker_duplicate_finder(\"seshat_info_Jul_22\", ALL_POLITIES=True)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_minutes = elapsed_time / 60\n",
    "\n",
    "print(\"Wiki Work: --- %s minutes ---\" % elapsed_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For analyzing unique refs\n",
    "\n",
    "from python_files import z_helpers, main_scraper\n",
    "file_folder_wiki = \"seshat_info_Jul_22\"\n",
    "file_folder_browser = \"seshat_browser_Jan_30_2023\"\n",
    "unique_refs_augmented_wiki, meta_data_wiki = z_helpers.ultimate_ref_dic_maker_plus(file_folder_wiki)\n",
    "unique_refs_augmented_browser, meta_data_browser = z_helpers.ultimate_ref_dic_maker_plus(file_folder_browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For analyzing unique refs on browser\n",
    "\n",
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "#file_folder = \"seshat_info_Jul_22\"\n",
    "file_folder = \"seshat_browser_Jan_30_2023\"\n",
    "\n",
    "all_unique_refs = z_helpers.analyze_the_augmented_json_file(file_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For analyzing all refs on wiki\n",
    "\n",
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "file_folder = \"seshat_info_Jul_22\"\n",
    "#file_folder = \"seshat_browser_Jan_30_2023\"\n",
    "\n",
    "all_unique_refs = z_helpers.analyze_the_augmented_json_file(file_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in all_unique_refs[230:280]:\n",
    "    print(item)\n",
    "    print(\"______\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Barfield Thomas Afghanistan A Cultural and Political History',\n",
    "'Barfield Thomas Afghanistan a cultural and political history',\n",
    "'(Barfield 2010) Thomas Barfield 2010 Afghanistan a cultural and political history Princeton University Press',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "augmented_json_folder = \"seshat_browser_Jan_30_2023\"\n",
    "augmented_json_file = \"a_dic_with_info_on_children_for_\" + augmented_json_folder + \".json\"\n",
    "with open(augmented_json_file) as json_file:\n",
    "    refs_dic = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_json_folder = \"seshat_browser_Jan_30_2023\"\n",
    "\n",
    "refs_json_file = \"a_dic_with_info_on_children_for_\" + refs_json_folder + \".json\"\n",
    "with open(refs_json_file) as json_file:\n",
    "    refs_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-sucking",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_columns = [\"polity\", \"variable\", \"page_from\", \"page_to\", \"trimmed\", \"original\", \"trimplus\"]\n",
    "long_df = pd.DataFrame(columns = df_columns)\n",
    "\n",
    "for kk, vv in refs_data.items():\n",
    "    for values in vv:\n",
    "        my_polity = kk.split(\"_\")[-1]\n",
    "        zotero_id = \"HAS_NO_ZOTERO_YET\"\n",
    "        if values[\"zoteroID\"]:\n",
    "            zotero_id = values[\"zoteroID\"][0]\n",
    "        new_values_dic = {\n",
    "                           \"polity\":  my_polity,\n",
    "                           \"variable\":  \"xxx\",\n",
    "                           \"page_from\":  values.get(\"page_from\"),\n",
    "                           \"page_to\":  values.get(\"page_to\"),\n",
    "                            #\"zotero\": zotero_id ,\n",
    "                           \"trimmed\":  values[\"trimmedText\"],\n",
    "                            \"original\": values[\"originalText\"], \n",
    "            \"trimplus\": values[\"trimmedTextPLUS\"],\n",
    "                        }\n",
    "        long_df = long_df.append(new_values_dic, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-brown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Set the text alignment for the columns\n",
    "short_df = long_df[[\"polity\", \"page_from\", \"page_to\", \"original\", \"trimmed\", \"trimplus\"]]\n",
    "short_df.style.set_properties(**{'text-align': 'left'})\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Calculates the Jaccard similarity between two strings.\n",
    "    \n",
    "    Args:\n",
    "    str1 (str): The first string to compare.\n",
    "    str2 (str): The second string to compare.\n",
    "    \n",
    "    Returns:\n",
    "    float: A value between 0 and 1 representing the Jaccard similarity between the two strings.\n",
    "    \"\"\"\n",
    "    str1 = re.sub(r'\\W+', ' ', str1).lower()\n",
    "    str2 = re.sub(r'\\W+', ' ', str2).lower()\n",
    "    a = set(str1.split())\n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    #print(str1)\n",
    "    #print(str2)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "for index, row in short_df.iterrows():\n",
    "    similarity = jaccard_similarity(row['trimplus'], \"Avari, Burjor. India: the ancient past: a history of the Indian sub-continent from c. 7000 BC to AD 1200. 2007,\")\n",
    "    if similarity > 0.6:\n",
    "        print(index, f\": ({similarity} and {row['polity']})\", row['trimplus'])\n",
    "#filtered_df = short_df[short_df['trimplus'].str.len() < 80 ]\n",
    "\n",
    "\n",
    "#filtered_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-chemical",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls = 0\n",
    "for kk, vv in refs_dic.items():\n",
    "    for item in vv:\n",
    "        if item[\"trimmedText\"].lower().count('; ')>2 and \"&amp;\" not in item[\"trimmedText\"].lower() and \"for a brief look at the decline\" not in item[\"trimmedText\"].lower():\n",
    "            urls+=1\n",
    "            print(item)\n",
    "            print(f\"______{kk}______\")\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Calculates the Jaccard similarity between two strings.\n",
    "    \n",
    "    Args:\n",
    "    str1 (str): The first string to compare.\n",
    "    str2 (str): The second string to compare.\n",
    "    \n",
    "    Returns:\n",
    "    float: A value between 0 and 1 representing the Jaccard similarity between the two strings.\n",
    "    \"\"\"\n",
    "    str1 = re.sub(r'\\W+', ' ', str1).lower()\n",
    "    str2 = re.sub(r'\\W+', ' ', str2).lower()\n",
    "    a = set(str1.split())\n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    print(str1)\n",
    "    print(str2)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "str1 = \"Barfield Thomas\"\n",
    "str2 = \"(Barfield 2010) Thomas\"\n",
    "\n",
    "print(\"Jaccard similarity: {:.5f}\".format(jaccard_similarity(str1, str2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-physics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-guide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-kingston",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "z_helpers.check_page_finders_checklist(z_helpers.check_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSON file into a pandas DataFrame\n",
    "df = pd.read_json('only_unique_citations_for_seshat_browser_Jan_30_2023.json')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "my_str = \"d Capiven: Yale University Press, 357.)\"\n",
    "a,b, c = z_helpers.find_pages_in(my_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-doctor",
   "metadata": {},
   "source": [
    "# Let's run the full nlp analysis\n",
    "\n",
    "### Improve the quality of the code and put it in a function so that we can take care of the most number of shorties and add second and third guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "\n",
    "#root_dir = \"seshat_browser_Jan_30_2023\"\n",
    "root_dir = \"seshat_info_Jul_22\"\n",
    "\n",
    "main_scraper.nlp_vars_html_extractor(root_dir, ALL_POLITIES=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"json_files/extra_ps_at_the_end_for_seshat_info_Jul_22.json\") as json_file:\n",
    "    extras = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"a_dic_with_info_on_children_for_seshat_info_Jul_22.json\") as json_file:\n",
    "    details = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk, vv in extras.items():\n",
    "    for pot_v in vv:\n",
    "        if \"McEvedy\" in pot_v:\n",
    "            print(kk, pot_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in details.items():\n",
    "    for index, item in enumerate(value):\n",
    "        if item[\"originalText\"] == \"SAME_AS_TRIMMED\":\n",
    "            x = item[\"trimmedText\"]\n",
    "        else:   \n",
    "            x = item[\"originalText\"]\n",
    "        if len(x) < 50  and len(x) >40 and item[\"hasPersonalComment\"] ==False:\n",
    "            print(x)\n",
    "            #print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-georgia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-davis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "\n",
    "shorties = 0\n",
    "good_shorties = 0\n",
    "good_shorties_all = 0\n",
    "two_hits = 0\n",
    "\n",
    "all_the_ps_in_all_polities = [] \n",
    "for kk, vv in extras.items():\n",
    "    for a_thing in vv:\n",
    "        all_the_ps_in_all_polities.append(a_thing)\n",
    "for key, value in details.items():\n",
    "    polity_name = key.split(\"_\")[-1]\n",
    "    #if polity_name == \"CnNWei*\":\n",
    "    #    print(key)\n",
    "    for index, item in enumerate(value):\n",
    "        if item[\"originalText\"] == \"SAME_AS_TRIMMED\":\n",
    "            x = item[\"trimmedText\"]\n",
    "        else:   \n",
    "            x = item[\"originalText\"]\n",
    "        #if polity_name == \"CnNWei*\":\n",
    "        #    print(x)\n",
    "        #    print(\"***********\")\n",
    "        if len(x) < 40:\n",
    "            shorties+=1\n",
    "            # find the year\n",
    "            the_year_plus_in_shortie =  z_helpers.find_the_year_plus(x)\n",
    "            the_top_ws_in_shortie =  z_helpers.first_three_words_finder(x)\n",
    "            the_ps_below = extras[polity_name]\n",
    "            \n",
    "            potential_hits = []\n",
    "            for a_p in the_ps_below:\n",
    "                number_of_hits = 0\n",
    "                num_of_words = len(the_top_ws_in_shortie)\n",
    "                for a_w in the_top_ws_in_shortie:\n",
    "                    if the_year_plus_in_shortie and the_year_plus_in_shortie[0] in a_p and len(a_w) >= 3 and a_w.lower() in a_p.lower():\n",
    "                        #print(f\"HIT: {x}  ----->  in {key} \\n {a_p}***\")\n",
    "                        #print(\"_____________\")\n",
    "                        number_of_hits+=1\n",
    "                        #potential_hits.append(a_p)\n",
    "                        #details[key][index][\"second_chance\"] = a_p\n",
    "                if number_of_hits == num_of_words:\n",
    "                    # high probability hit:\n",
    "                    potential_hits.append(a_p)\n",
    "                    #details[key][index][\"second_chance_high\"] = a_p\n",
    "                elif number_of_hits == 2 and num_of_words ==3 and a_p not in potential_hits:\n",
    "                    # 66% highly probale\n",
    "                    potential_hits.append(a_p)\n",
    "                    #details[key][index][\"second_chance_medium\"] = a_p\n",
    "            if len(potential_hits) == 1:\n",
    "                # Very likely hit:\n",
    "                good_shorties+=1\n",
    "                details[key][index][\"second_chance_high\"] = a_p\n",
    "            elif len(potential_hits) == 0:\n",
    "                print(f\"NOOOOOOOOOOOO HITs in {key}: {potential_hits}\")\n",
    "                two_hits+=1\n",
    "                print(\"_____________\")\n",
    "                    \n",
    "            \n",
    "            # We need to fo a similar approach for third hits (from all polities)        \n",
    "            for a_p in all_the_ps_in_all_polities:\n",
    "                for a_w in the_top_ws_in_shortie:\n",
    "                    if the_year_plus_in_shortie and \"second_chance\" not in details[key][index].keys() and the_year_plus_in_shortie[0] in a_p and len(a_w) > 4 and a_w.lower() in a_p.lower():\n",
    "                        #print(f\"HIT  +++++ : {x}  ----->  in {key} \\n {a_p}***\")\n",
    "                        #print(\"_____________\")\n",
    "                        good_shorties_all+=1\n",
    "                        details[key][index][\"second_chance_from_all_pols\"] = a_p\n",
    "                        break            #print(x)\n",
    "\n",
    "print(good_shorties, \" out of \", shorties, \" had a Hit.\")\n",
    "print(two_hits, \" out of \", shorties, \" had two Hits.\")\n",
    "\n",
    "print(good_shorties_all, \" out of \", shorties, \" had a third Hit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "badies = 0\n",
    "for key, value in details.items():\n",
    "    for item in value:\n",
    "        if len(item[\"trimmedText\"]) < 40 and \"second_chance\" in item.keys() and \"second_chance_from_all_pols\" in item.keys():\n",
    "            print(key, item)\n",
    "            badies+=1\n",
    "print(badies)\n",
    "#details[\"REF_567_CnEHan*\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "\n",
    "#file_dir = \"seshat_browser_Jan_30_2023\"\n",
    "file_dir = \"seshat_info_Jul_22\"\n",
    "my_details = z_helpers.find_second_and_third_guess(file_dir, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for kk, vv in my_details.items():\n",
    "    for pot_v in vv:\n",
    "        if \"second_chance\" not in pot_v.keys() and \"third_chance\" not in pot_v.keys() and len(pot_v[\"trimmedText\"]) < 50:\n",
    "            print(kk)\n",
    "            print(pot_v[\"trimmedText\"])\n",
    "            count+=1\n",
    "            print(\"_____\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk, vv in my_details.items():\n",
    "    for pot_v in vv:\n",
    "        if \"second_chance\" in pot_v.keys() or \"third_chance\" in pot_v.keys():\n",
    "            print(kk)\n",
    "            print(vv)\n",
    "            print(\"_____\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [\"b\", \"c\", \"d\"]\n",
    "if \"b\" in a:\n",
    "    a.remove(\"b\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Gabriel (2003:)\"\n",
    "\"(Lewis 1999b)\"\n",
    "\"(Lorge, 2012)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def first_n_words_finder(a_str, n):\n",
    "    \"\"\"\n",
    "    Finds the first three words in a string (or the first two, or the first one)\n",
    "    \"\"\"\n",
    "    regex = r\"\\b([a-zA-Z\\u00C0-\\u017F']+)\\b(?:\\W+\\b([a-zA-Z\\u00C0-\\u017F']+)\\b)?(?:\\W+\\b([a-zA-Z\\u00C0-\\u017F']+)\\b)?(?:\\W+\\b([a-zA-Z\\u00C0-\\u017F']+)\\b)?\"\n",
    "    match = re.search(regex, a_str)\n",
    "    if match:\n",
    "        words = [match.group(i) for i in range(1, n+1) if match.group(i)]\n",
    "        return words\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_n_words_finder(\"(Lewüis ali maün, neda 1999b)\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-planner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_files import z_helpers, main_scraper\n",
    "\n",
    "z_helpers.first_three_words_finder(\"(Hulsewé 1986, 543)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-harvest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "extras[\"CnEHan*\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-appendix",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
